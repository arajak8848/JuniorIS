% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.

% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.
@incollection{vidal2022optimization,
  author       = {Rene, Vidal and Zhihui Zhu and Benjamin D. Haeffele},
  title        = {Optimization Landscape of Neural Networks},
  booktitle    = {Mathematical Aspects of Deep Learning},
  editor       = {Philipp Grohs and Gitta Kutyniok},
  publisher    = {Cambridge University Press},
  year         = {2022},
  pages        = {200--228},
  doi          = {10.1017/9781009025096.005},
  url          = {https://www.cambridge.org/core/services/aop-cambridge-core/content/view/E8E68EA489DC86BC75013421EBBF824C/stamped-9781316516782c4_200-228.pdf/optimization_landscape_of_neural_networks.pdf},
  annote         = {
    Many tasks in machine learning involve solving a convex optimization
    problem which significantly facilitates the analysis of properties of the resulting
    algorithms, such as their optimality, robustness, and generalization. An important
    challenge in training neural networks occurs when the associated optimization
    problem is non-convex; this complicates the analysis because global optima can be
    difficult to characterize and the optimization landscape can also include spurious
    local minima and saddle points. As a consequence, different algorithms might
    attract different weights depending on initialization, parameter tuning, etc. Despite
    this challenge, in practice existing algorithms routinely converge to good solutions,
    which suggests that the landscape might be simpler than expected, at least for certain
    classes of networks.
  },
}

@article{Shor1997,
  author    = {Shor, Peter W.},
  title     = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
  journal   = {SIAM Journal on Computing},
  volume    = {26},
  number    = {5},
  pages     = {1484--1509},
  year      = {1997},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://www2.math.upenn.edu/~ted/210S14/References/Shor.MAN.pdf},
  annote ={
    The Church-Turing thesis says that a digital computer is a
    universal computational device; that is, it is able to simulate any physically realizable computational device. It has generally been believed that
    this simulation can be made efficient so that it entails at most a polynomial increase in computation time. This may not be true if quantum
    mechanics is taken into consideration. A quantum computer is a hypothetical machine based on quantum mechanics. We explain quantum
    computing, and give an algorithm for prime factorization on a quantum
    computer that runs asymptotically much faster than the best known algorithm on a digital computer. It is not clear whether it will ever be
    possible to build large-scale quantum computers. One of the main difficulties is in manipulating coherent quantum states without introducing
    errors or losing coherence. We discuss quantum error-correcting codes
    and fault-tolerant quantum computing, which can guarantee highly reliable quantum computation, given only moderately reliable quantum
    computing hardware.
  }
}

@article{Min2017,
  author    = {Min, Seonwoo and Lee, Byunghan and Yoon, Sungroh},
  title     = {Deep Learning in Bioinformatics},
  journal   = {Briefings in Bioinformatics},
  volume    = {18},
  number    = {5},
  pages     = {851--869},
  year      = {2017},
  publisher = {Oxford University Press},
  url       = {https://academic.oup.com/bib/article/18/5/851/2562742},
  note      = {Accessed: January 29, 2026},
  annote    = {
    This review paper surveys the application of deep learning 
    methods to major bioinformatics challenges arising from 
    large-scale biomedical data. The authors categorize existing 
    research by both application domain, including omics 
    analysis, biomedical imaging, and signal processing, 
    and by neural network architecture, such as deep neural 
    networks, convolutional neural networks, and recurrent 
    neural networks. Representative studies are summarized 
    to illustrate how deep learning models extract meaningful 
    patterns from complex biological data.

    The source is credible and authoritative, as it is published 
    in the peer-reviewed journal Briefings in Bioinformatics 
    and written by established researchers in machine learning 
    and bioinformatics. As a review article, it synthesizes a 
    broad range of prior work and provides a balanced discussion 
    of both successes and limitations.

    This paper is relevant to research interests in applying 
    machine learning to biological data, offering a structured 
    overview of current approaches and highlighting key 
    challenges and future research directions in the field.
}
}



