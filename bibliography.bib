% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.

% Here is an example of how to create a bibliography entry for an article using
% BibTeX. Generally you won't have to write these out yourself, because they are
% provided by most web sites that allow you to export citations. The string
% "clrsAlgorithms" is a citation key, and if you were citing the source in a
% document you would use \cite{clrsAlgorithms}.
@incollection{vidal2022optimization,
  author       = {Rene, Vidal and Zhihui Zhu and Benjamin D. Haeffele},
  title        = {Optimization Landscape of Neural Networks},
  booktitle    = {Mathematical Aspects of Deep Learning},
  editor       = {Philipp Grohs and Gitta Kutyniok},
  publisher    = {Cambridge University Press},
  year         = {2022},
  pages        = {200--228},
  doi          = {10.1017/9781009025096.005},
  url          = {https://www.cambridge.org/core/books/mathematical-aspects-of-deep-learning/optimization-landscape-of-neural-networks/E8E68EA489DC86BC75013421EBBF824C},
  annote         = {
    Many tasks in machine learning involve solving a convex optimization
    problem which significantly facilitates the analysis of properties of the resulting
    algorithms, such as their optimality, robustness, and generalization. An important
    challenge in training neural networks occurs when the associated optimization
    problem is non-convex; this complicates the analysis because global optima can be
    difficult to characterize and the optimization landscape can also include spurious
    local minima and saddle points. As a consequence, different algorithms might
    attract different weights depending on initialization, parameter tuning, etc. Despite
    this challenge, in practice existing algorithms routinely converge to good solutions,
    which suggests that the landscape might be simpler than expected, at least for certain
    classes of networks.
  },
}

@article{Shor1997,
  author    = {Shor, Peter W.},
  title     = {Polynomial-Time Algorithms for Prime Factorization and Discrete Logarithms on a Quantum Computer},
  journal   = {SIAM Journal on Computing},
  volume    = {26},
  number    = {5},
  pages     = {1484--1509},
  year      = {1997},
  publisher = {Society for Industrial and Applied Mathematics},
  url       = {https://www2.math.upenn.edu/~ted/210S14/References/Shor.MAN.pdf},
  annote ={
    The Church-Turing thesis says that a digital computer is a
    universal computational device; that is, it is able to simulate any physically realizable computational device. It has generally been believed that
    this simulation can be made efficient so that it entails at most a polynomial increase in computation time. This may not be true if quantum
    mechanics is taken into consideration. A quantum computer is a hypothetical machine based on quantum mechanics. We explain quantum
    computing, and give an algorithm for prime factorization on a quantum
    computer that runs asymptotically much faster than the best known algorithm on a digital computer. It is not clear whether it will ever be
    possible to build large-scale quantum computers. One of the main difficulties is in manipulating coherent quantum states without introducing
    errors or losing coherence. We discuss quantum error-correcting codes
    and fault-tolerant quantum computing, which can guarantee highly reliable quantum computation, given only moderately reliable quantum
    computing hardware.
  }
}


